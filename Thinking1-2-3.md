### 什么是反向传播中的链式法则
* BP是应用于多层隐式神经网络，其本质是一个，关于函数的函数，也就是复合函数。
* 在反向过程中，利用误差的梯度，来调节神经元之间的权值。求梯度，就需要利用求导。
* 数学定理：复合函数的导数，可用构成复合函数的各个函数的导数乘积来表示。就像一个链条（chain）连接起来一样。
* 神经网络权值更新的计算复杂度，从O(n2) 下降到O(n)。
### 请列举几种常见的激活函数，激活函数有什么作用
* 如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。
* 如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。
<br>
	常见的激活函数：

* sigmoid：
  - 优点：取值范围为(0,1)， 它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。
  - 缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法，反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练

  
* tanh
  - 优点：比Sigmoid函数收敛速度更快。相比Sigmoid函数，其输出以0为中心。
  - 缺点：没有改变Sigmoid函数的最大问题, 由于饱和性产生的梯度消失。

* ReLU
  - 相比起Sigmoid和tanh，ReLU在SGD中能够快速收敛。
  - Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。
  - 有效缓解了梯度消失的问题。
  - 在没有无监督预训练的时候也能有较好的表现。
  - 提供了神经网络的稀疏表达能力。
  - 缺点：随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。

* ELU和 Leak ReLU都是ReLU的变种。


### 利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决。？

可能的loss不变的问题：
1. 模型结构和特征工程存在问题，当模型结构不好或者规模太小、特征工程存在问题时，其对于数据的拟合能力不足。解决方式：通过不断的测试不同的结构以及特征工程方案，进行改进和适应性修改。
2. 要根据不同的情况，选择合适的激活函数、损失函数。
  - 积神经网络中，卷积层的输出，一般使用ReLu作为激活函数，因为可以有效避免梯度消失，并且线性函数在计算性能上面更加有优势。而循环神经网络中的循环层一般为tanh，或者ReLu，全连接层也多用ReLu，只有在神经网络的输出层，使用全连接层来分类的情况下，才会使用softmax这种激活函数
  - 损失函数，对于一些分类任务，通常使用交叉熵损失函数，回归任务使用均方误差
3. 梯度消失、神经元失活、梯度爆炸和弥散、学习率过大或过小
  - 梯度消失：可以通过梯度的检验来验证模型当前所处的状态
  - 神经元失活，Relu激活函数的时候，当每一个神经元的输入X为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。可以使用Leak ReLU
  - 梯度爆炸和梯度弥散产生的根本原因是，根据链式法则，深度学习中的梯度在逐层累积。如1.1的n次方无穷大，0.9的n次方无穷小。解决方案：为输出取一个上界，可用最大范数约束。
4. 未进行归一化
